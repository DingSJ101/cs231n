# Assignment one 

goal :
In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:

understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)
understand the train/val/test splits and the use of validation data for hyperparameter tuning.
develop proficiency in writing efficient vectorized code with numpy
implement and apply a k-Nearest Neighbor (kNN) classifier
implement and apply a Multiclass Support Vector Machine (SVM) classifier
implement and apply a Softmax classifier
implement and apply a Two layer neural network classifier
understand the differences and tradeoffs between these classifiers
get a basic understanding of performance improvements from using higher-level representations than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)

## task one : k-Nearest Neighbor classifier (20 points)
The IPython Notebook knn.ipynb will walk you through implementing the kNN classifier.

输入形状为$( num\_test \times D) $的`X`，输出维度为$(num\_test \times num\_train)$的矩阵dists，其中dists[i,j]表示X[i]与X_train[j]的欧氏距离。
欧氏距离(Euclidean Distance) : P维向量I1和I2的欧式距离为p个分量的差的均方根。
$$
d_{L2}(I_1,I_2) = \sqrt{\sum_{p}{(I^P_1-I^P_2)^2}}
$$

$$
A = X  : (num\_test,P) \\
B = X\_train : (num\_train,P)\\
Dists_{i,j} = \sqrt{\sum_{p=1}^{P}{(A_{i,p}-B_{j,p})^2}}\\
=\sqrt{\sum_{p}{[(A_{i,p})^2}+(B_{j,p})^2-2*(A_{i,p})*(B_{j,p})]}\\
=\sqrt{\sum_{p}{(A_{i,p})^2}+\sum_{p}{(B_{j,p})^2}-2*\sum_{p}{(A_{i,p})*(B_{j,p})}}\\
=\sqrt{\sum_{p}{(A_{i,p})^2}+\sum_{p}{(B^T_{p,j})^2}-2*\sum_{p}{(A_{i,p})*(B^T_{p,j})}}\\


$$

### Process one : implement function `KNerestNeighbor.compute_distance_two_loops` in `k_nearest_neighbor.py`
```python 
for i in range(num_test):
    for j in range(num_train):
        dists[i][j] =  np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2))
```

### Process two : implement function `KNerestNeighbor.predict_labels` in `k_nearest_neighbor.py`
输入形状为$(num\_test*num\_train)$的dists，输出$(num\_test,)$的向量y_pred，其中y_pred[i]表示第i个测试样本的预测标签。
思路： 遍历每个待预测样本，假设为第i个，则遍历dists[i,:]（即该样本到训练集每个样本的距离），找到最小的前k个。接着从这k个样本对应标签中选择作为预测结果的标签，这里直接取众数。
```python 
closest_y = self.y_trian[np.argsort(dists[i])[0:k]]#从小到大依次排序dists 把他们的坐标存入y_train也就是标签中
y_pred[i] = np.argmax(np.bincount(closest_y)) #bincount输出形状为(max(clostsest_y)+1,1)的向量L，其中L[i]表示正整数i出现的次数，L的最大值下标L.argmax()表示L中出现最多的数
```

### Process three : Implement the function `KNerestNeighbor.compute_distances_one_loop` in `k_nearest_neighbor.py`
```python 
for i in range(num_test):
    dists[i][j] =  np.sqrt(np.sum((X[i] - self.X_train) ** 2,1))# 使用广播机制，简化重复匹配过程
```

### Process four : Implement the function `KNerestNeighbor.compute_distances_no_loop` in `k_nearest_neighbor.py`
```python 
# 公式见前述，使用两次广播分别计算A和B，最后的差项通过矩阵乘法获得

a = np.sum(np.square(X), axis=1, keepdims=True)
b = np.sum(np.square(self.X_train), axis=1, keepdims=True).T
dists = np.sqrt(a + b - 2 * X.dot(self.X_train.T))
```

### Process five : Implement the X-fold cross-validation in `knn.ipynb`
将训练集k折拆分，并进行交叉验证。
```python
X_train_folds = np.split(X_train,num_folds)
y_train_folds = np.split(y_train,num_folds)
k_to_accuracies = {}
for k in k_choices:
    classifier = KNearestNeighbor()
    k_to_accuracies[k] = []
    for i in range(num_folds):
        x_train_fold = np.concatenate([fold for j,fold in enumerate(X_train_folds) if i!=j ])
        y_train_fold = np.concatenate([fold for j,fold in enumerate(y_train_folds) if i!=j ])
        
        classifier.train(x_train_fold, y_train_fold)
        y_pred_fold = classifier.predict(X_train_folds[i], k=k, num_loops=0)
        num_correct = np.sum(y_pred_fold == y_train_folds[i])
        accuracy = float(num_correct) / X_train_folds[i].shape[0]
        k_to_accuracies[k].append(accuracy)
```


## task two : Training a Support Vector Machine (25 points)
The IPython Notebook svm.ipynb will walk you through implementing the SVM classifier.
 
 In this exercise you will:
    
- implement a fully-vectorized **loss function** for the SVM
- implement the fully-vectorized expression for its **analytic gradient**
- **check your implementation** using numerical gradient
- use a validation set to **tune the learning rate and regularization** strength
- **optimize** the loss function with **SGD**
- **visualize** the final learned weights

### Process one ；Implement the gradient inside function `svm_loss_naive`in `linear_svm.py`
输入形状为$(D\times C)$的W，$(N\times D)$的X，$(N\times 1)$的y，其中D为样本维度，N为批量大小，C为标签类别数  
输出元组（loss,grad），其中grad形状与W相同  
Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 <= c < C.
  - reg: (float) regularization strength  
$S = X\cdot W +b$  
$x_i表示第i个样本向量 \\ w^T_j表示w矩阵的第j列$

第i个样本的损失函数如下：
$$
L_i(x_i;W) = \sum^{C}_{j\not=y_i}{max(0,score_j-score_{y_i}+\Delta)}\\
=\sum^{C}_{j\not=y_i}{max(0, x_i\cdot w^T_j-x_i\cdot w^T_{y_i}+\Delta)}
$$
总的损失函数如下：
$$
L(x;W) = \frac{1}{N}\sum\limits_{i}{L_i(x_i;W)}\\
=\frac{1}{N}\sum_i^{N}\sum_{j\not=y_i}^{C}{{max(0,score_j-score_{y_i}+\Delta)}}
$$
为简化计算过程，设$score_j-score_{y_i}+\Delta >=0$，则有：
$$
L(x;W) = \frac{1}{N}\sum\limits_{i}{L_i(x_i;W)}\\
=\frac{1}{N}\sum_i^{N}\sum_{j\not=y_i}^{C}{{(score_j-score_{y_i}+\Delta)}} \\ 
=\frac{1}{N}\sum_i^{N}\sum_{j\not=y_i}^{C}{{(x_i\cdot w^T_{j}-x_i\cdot w^T_{y_i}+\Delta)}}
$$
考虑$x = [x_1,x_2,\dots,x_N] ,W = [w_1^T,w_2^T,\dots,w_C^T]^T $ ,接下来进行标量L对矩阵W求导
$$
\dfrac{\partial L}{\partial W} 
= \frac{1}{N}\sum_{i=1}^{N}\sum_{j\not= y_i}^{C}
    \left( 
            \begin{bmatrix}
                \dfrac{\partial x_i\cdot w_j^T}{\partial w_1^T},
                \dfrac{\partial x_i\cdot w_j^T}{\partial w_2^T},
                \cdots,
                \dfrac{\partial x_i\cdot w_j^T}{\partial w_C^T}
            \end{bmatrix}^T
    %\right. \\ \left. 
    -
            \begin{bmatrix}
                \dfrac{\partial x_i\cdot w^T_{y_i}}{\partial w_1^T},
                \dfrac{\partial x_i\cdot w^T_{y_i}}{\partial w_2^T},
                \cdots,
                \dfrac{\partial x_i\cdot w^T_{y_i}}{\partial w_C^T}
            \end{bmatrix}^T
    \right) 
$$
注意到 $
\left\{
\begin{array}{}
    \dfrac{\partial x_k\cdot w_i^T}{\partial w_j^T} = 0 &, i\not=j \\
    \dfrac{\partial x_k\cdot w_i^T}{\partial w_j^T} = x_k & ,i=j \\
\end{array}
\right. $,则：
$$
\dfrac{\partial L}{\partial W}  = \frac{1}{N}
\left(
    \left [
        \sum_{i=1}^{N}{x_i|_{1\not=y_i}},\cdots,\sum_{i=1}^{N}{x_i|_{C\not=y_i}}
    \right ]^T
    -
    \left [
        \sum_{i=1}^{N}\sum_{j\not=y_i}^{C}x_i|_{y_i=1},\cdots,\sum_{i=1}^{N}\sum_{j\not=y_i}^{C}x_i|_{y_i=C}
    \right ]^T
\right)
$$
注：$x_i|_{y_i=k}$表示当i取某个特定值使得$y_i=k$成立时，此项取$x_i$，不然取0。说人话就是此表达式后一个向量中的第i项是X中所有属于第i个类别的样本$x_i$的和。

损失函数的正则项惩罚为$R(W)=||W||=\sum\limits_{k}\sum\limits_{l}W^2_{k,l}$,对应的正则项为$\lambda R(W)$,梯度为$d\lambda R(W) = \lambda d||W|| = 2\lambda W$

```python
for i in range(num_train):
    scores = X[i].dot(W)  # (1,D)*(D,C)
    correct_class_score = scores[y[i]]
    for j in range(num_classes):
      if j == y[i]:
        continue
      margin = scores[j] - correct_class_score + 1 # note delta = 1
      if margin > 0:
        dW[:, y[i]] -= X[i] # dW[:,1].shape=(D,),即一维向量，1*D
        dW[:, j] += X[i] 
        loss += margin
loss /= num_train
loss += reg * np.sum(W * W)
dW /= num_train
dW += 2 * W * reg
```

### process two : Implement the loss inside function `svm_loss_vectorized`in `linear_svm.py`

```python
scores = X.dot(W) # N, C
num_train = X.shape[0]
correct_class_score = scores[np.arange(num_train), y] # 使用数组索引,下标为(i,y_i) , shape: (num_class,)   
correct_class_score = correct_class_score.reshape([-1, 1]) # shape: (num_class,1) 
loss_tep = scores - correct_class_score + 1
loss_tep[loss_tep < 0] = 0  # max(0,loss_tep)
# loss_tep = np.maximum(0, loss_tep)
loss_tep[np.arange(num_train), y] = 0 # 正确的类loss为0
loss = loss_tep.sum()/num_train + reg * np.sum(W * W)

```

### process three : Implement the gradient inside function `svm_loss_vectorized`in `linear_svm.py`
$\frac{\partial L}{\partial W}$与$W$形状相同，$\frac{\partial L}{\partial W}$中元素$\frac{\partial L}{\partial W}_{i,j}$表示$W_{i,j}$对L的贡献。


$记损失函数为L = f_1(S),S = f_2(x;W)，其中\left\{ \begin{array}{l} L = \sum\limits_{i,j}S_{i,j} \\ S_{i,j}=max(score_i-score_{y_i} + \Delta) \end{array} \right. 则：\\
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial S}\cdot \frac{\partial S}{\partial W},S为Score矩阵，S、W和\frac{\partial L}{\partial W}的形状都相同。\\
由于L = ||S||_1 = \sum\limits_{i,j}S_{i,j} , \dfrac{\partial L}{\partial S} =? , \dfrac{\partial S}{\partial W} = X^T , 故 \dfrac{\partial L}{\partial W} = X^T\cdot $


$\frac{\sum\limits_{i,j}^{}{x_{i,j}}\partial x_i-x_{y_i}}{\partial W_{i,j}}$
```python 
# loss_tep等于0的位置，对X的导数就是0
# loss_tep元素大于0的位置，说明此处有loss，对于非正确标签类的S求导为1
loss_tep[loss_tep > 0] = 1  # N, C  
# 对于正确标签类，每有一个loss_tep元素大于0，则正确标签类的S求导为-1，要累加
loss_item_count = np.sum(loss_tep, axis=1)     
loss_tep[np.arange(num_train), y] -= loss_item_count  #在一次错误的分类中，
# dW中第i,j元素对应于第i维，第j类的权重
# X.T的第i行每个元素对应于每个样本第i维的输入，正是Sj对W[i,j]的导数
# loss_tep的第j列每个元素对应于每个样本在第j类的得分是否出现，相当于掩码
# X.T和loss_tep的矩阵乘法等于对每个样本的W[i,j]导数值求和
# 简单地说，就是loss对S的导数是loss_tep， loss_tep对W的导数是X.T
dW = X.T.dot(loss_tep) / num_train  # (D, N) *(N, C) 
dW += 2 * reg * W
```

### process four : Implement the SGD and predict in classifier `LinearClassifier()` in `linear_classifier.py`
LinearClassifier.train(X,y)  
输入形状为(N,D)的X 和 形状为(N,)的y表示训练样本和标签  
使用随机梯度下降训练self.W

```python 
batch_idx = np.random.choice(num_train, batch_size) #随机选5个
X_batch = X[batch_idx,:]
y_batch = y[batch_idx]
self.W += -learning_rate*grad
```

LinearClassifier.predict(X)  
输入形状为(N,D)的X 表示训练样本  
输出形状为(N,)的y_pred

```python 
y_pred = X.dot(self.W).argmax(axis = 1)
```

### process five : Implement cell in `svm.ipynb` to get the best hyperparameters
代码较简单。给出的学习率和正则项范围过大（运算时会发生NAN），需要自行修改。
```python 
learning_rates = [2e-7,0.75e-7,1.5e-7,1.25e-7,0.75e-7]
regularization_strengths = [3e4,3.25e4,3.5e4,3.75e4,4e4]
for lr in learning_rates:
    for reg in regularization_strengths:
        svm = LinearSVM()
        svm.train(X_train,y_train,learning_rate = lr,reg = reg,num_iters=1000,verbose=True)
        y_train_pred = svm.predict(X_train)
        y_val_pred = svm.predict(X_val)
        accuracy_train = np.mean(y_train_pred == y_train)
        accuracy_val = np.mean(y_val_pred == y_val)
        if best_val<accuracy_train:
            best_val = accuracy_train
            best_svm = svm
        results[(lr,reg)] = (accuracy_train,accuracy_val)
```

## task three : Implement a Softmax classifier (20 points)
The IPython Notebook softmax.ipynb will walk you through implementing the Softmax classifier.

This exercise is analogous to the SVM exercise. You will:
- implement a fully-vectorized **loss function** for the Softmax classifier
- implement the fully-vectorized expression for its **analytic gradient**
- **check your implementation** with numerical gradient
- use a validation set to **tune the learning rate and regularization** strength
- **optimize** the loss function with **SGD**
- **visualize** the final learned weights




### process one : Implement loss function `softmax_loss_naive` in `softmax.py`  
Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) 
  - reg: (float) regularization strength  

Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W

softmax算法：对于维度为N的向量`s`，通过softmax函数，可以将每个维度映射到[0,1]区间内的实数。

$$
softmax(s_i) = \dfrac{\Large 
e^{s_i}}{\sum\limits_{c}{\Large 
e^{s_c}}} 
$$

对于多分类问题中，第`i`个样本的输出向量为`s`，标签向量为`y`，则该样本的损失函数$L_i$为 $L_i = -log{P(Y=y_i|X=x_i)}$，其中概率P使用交叉熵进行计算，则有
$$
L_i = -log{P(Y=y_i|X=x_i)} \\
= -log(softmax(s_{y_i})) \\
=-log(\dfrac{\Large e^{s_{y_i}}}{\sum\limits_{c}{\Large 
e^{s_c}}}) \\
= -s_{y_i} + log\Large\sum\limits_c{e^{s_c}}
$$  

相应的梯度
$$dW = \dfrac{\partial L}{\partial S}\cdot \dfrac{\partial S}{\partial W} \\
\left\{\begin{array}{l}
\dfrac{\partial L_i}{\partial S} =[\cdots,0,-1|_{position = {y_i}},0,\cdots] + \dfrac{1}{\sum\limits_c{e^{s_c}}} \cdot [e^{s_1},\cdots,e^{s_c}] \\  
\dfrac{\partial S}{\partial W} =  \dfrac{\partial W\cdot X}{\partial W} = X^T
\end{array}\right .
$$

```python 
N = X.shape[0]
for i in range(N):
    score = X[i].dot(W)
    exp_score = np.exp(score-score.max())
    loss += -np.log(exp_score[y[i]]/np.sum(exp_score))

    dexp_score = exp_score/np.sum(exp_score)
    dexp_score[y[i]]-=1
    dW += X[[i]].T.dot([dexp_score])
loss/= N
loss += reg*np.sum(W**2)
dW/=N
dW+=2*reg*W
```

### process two : Implement loss function `softmax_loss_vectorized` in `softmax.py` 


```python
N = X.shape[0]
scores = X.dot(W)  # (N, C)
scores1 = scores - np.max(scores, axis=1, keepdims=True)  # (N, C)
loss1 = -scores1[range(N), y] + np.log(np.sum(np.exp(scores1), axis=1))  # (N,)
loss = np.sum(loss1) / N + reg * np.sum(W ** 2)

dloss1 = np.ones((N, 1)) / N  # (N, 1)
dscores = dloss1 * np.exp(scores1) / np.sum(np.exp(scores1), axis=1, keepdims=True)  # (N, C)
dscores[[range(N)], y] -= dloss1.T # (N, C)
dW = X.T.dot(dscores) + 2 * reg * W
```

### process three :Implement cell in `softmax.ipynb` to get the best hyperparameters
代码基本同SVM ,修改LinearSVM()为Softmax()
```python 
learning_rates = [1e-7, 2e-6, 2.5e-6]
regularization_strengths = [1e3, 1e4, 2e4, 2.5e4, 3e4, 3.5e4]
```


## task four : Two-Layer Neural Network (25 points)
The IPython Notebook two_layer_net.ipynb will walk you through the implementation of a two-layer neural network classifier.


### process one :Implement `TwoLayerNet.loss` in `neural_net.py` 


### process two :Implement `TwoLayerNet.train` in `neural_net.py` 

### process three :Implement `TwoLayerNet.train` in `neural_net.py` 

### process four : get the best hyperparameters in `two_layer_net.ipynb`



## task five : Higher Level Representations: Image Features (10 points)
The IPython Notebook features.ipynb will walk you through this exercise, in which you will examine the improvements gained by using higher-level representations as opposed to using raw pixel values.

略

##  Cool Bonus: Do something extra! (+10 points)
Implement, investigate or analyze something extra surrounding the topics in this assignment, and using the code you developed. For example, is there some other interesting question we could have asked? Is there any insightful visualization you can plot? Or anything fun to look at? Or maybe you can experiment with a spin on the loss function? If you try out something cool we’ll give you up to 10 extra points and may feature your results in the lecture.